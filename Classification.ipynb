{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cuda: False \n",
      " Apple MPS: True\n"
     ]
    }
   ],
   "source": [
    "print(' Cuda:', torch.cuda.is_available(), \"\\n\" ,'Apple MPS:', torch.backends.mps.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full pytorch workflow with MNIST fashion dataset used in the Pytorch learning path at Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU use\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # prefers GPU over CPU\n",
    "\n",
    "# Increases accuracy to 71 % compared to transform = ToTensor(), which delivers 41 %.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform)\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NeuralNetwork                            [1, 10]                   --\n",
       "├─Flatten: 1-1                           [1, 784]                  --\n",
       "├─Sequential: 1-2                        [1, 10]                   --\n",
       "│    └─Linear: 2-1                       [1, 512]                  401,920\n",
       "│    └─ReLU: 2-2                         [1, 512]                  --\n",
       "│    └─Linear: 2-3                       [1, 512]                  262,656\n",
       "│    └─ReLU: 2-4                         [1, 512]                  --\n",
       "│    └─Linear: 2-5                       [1, 10]                   5,130\n",
       "│    └─ReLU: 2-6                         [1, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 669,706\n",
       "Trainable params: 669,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.67\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 2.68\n",
       "Estimated Total Size (MB): 2.69\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "model = NeuralNetwork().to(device) # to device does not work here. see next cell\n",
    "summary(model,input_size=(1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# the addition of momentum = .9 improves the accuracy from 80 % to 84 % in 1 epoch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):  \n",
    "        # possible representation of (X, y) with more lines of code.      \n",
    "        # is just (X, y) = data an in additional line\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # zero your gradients for every batch!\n",
    "        loss.backward()\n",
    "        optimizer.step() # gradient descent\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(device, dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad(): # disables gradient calculation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296759  [    0/60000]\n",
      "loss: 0.561553  [ 4000/60000]\n",
      "loss: 0.322416  [ 8000/60000]\n",
      "loss: 1.399738  [12000/60000]\n",
      "loss: 2.023489  [16000/60000]\n",
      "loss: 1.163917  [20000/60000]\n",
      "loss: 0.661505  [24000/60000]\n",
      "loss: 2.435496  [28000/60000]\n",
      "loss: 1.831609  [32000/60000]\n",
      "loss: 0.149050  [36000/60000]\n",
      "loss: 1.230498  [40000/60000]\n",
      "loss: 0.364594  [44000/60000]\n",
      "loss: 0.635265  [48000/60000]\n",
      "loss: 0.582289  [52000/60000]\n",
      "loss: 0.616623  [56000/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.223090 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "def main():\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loop(device, test_dataloader, model, loss_fn)\n",
    "    print(\"Done\")\n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU runtime\n",
    "https://discuss.pytorch.org/t/sequential-throughput-of-gpu-execution/156303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.899077  [    0/60000]\n",
      "loss: 0.108631  [ 4000/60000]\n",
      "loss: 0.132975  [ 8000/60000]\n",
      "loss: 1.276919  [12000/60000]\n",
      "loss: 2.291896  [16000/60000]\n",
      "loss: 1.218495  [20000/60000]\n",
      "loss: 0.570299  [24000/60000]\n",
      "loss: 1.888932  [28000/60000]\n",
      "loss: 1.804555  [32000/60000]\n",
      "loss: 0.087260  [36000/60000]\n",
      "loss: 1.171256  [40000/60000]\n",
      "loss: 0.306714  [44000/60000]\n",
      "loss: 0.595607  [48000/60000]\n",
      "loss: 0.579024  [52000/60000]\n",
      "loss: 0.029249  [56000/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.164483 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if not torch.backends.mps.is_available() else \"cpu\") # prefers CPU over GPU\n",
    "epochs = 1\n",
    "def main():\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loop(device, test_dataloader, model, loss_fn)\n",
    "    print(\"Done\")\n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('data/model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.zeros((1,28,28))\n",
    "onnx_model = 'data/model.onnx'\n",
    "torch.onnx.export(model, input_image, onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "x, y = test_data[0][0], test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"fashion_mnist_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "session = onnxruntime.InferenceSession(onnx_model, None)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "result = session.run([output_name], {input_name: x.numpy()})\n",
    "predicted, actual = classes[result[0][0].argmax(0)], classes[y]\n",
    "print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading your own images\n",
    "\n",
    "In most of the practical applications, you would have your own images located on disk that you want to use to train your neural network. In this case, you need to load them into PyTorch tensors. \n",
    "\n",
    "One of the ways to do that is to use one of the Python libraries for image manipulation, such as *Open CV*, or *PIL/Pillow*, or *imageio*. Once you load your image into numpy array, you can easily convert it to tensors. \n",
    "\n",
    "> It is important to make sure that all values are scaled to the range [0..1] before you pass them to a neural network - it is the usual convention for data preparation, and all default weight initializations in neural networks are designed to work with this range. `ToTensor` transform that we have seen above automatically scales PIL/numpy images with integer pixel values into [0..1] range.\n",
    "\n",
    "Even better approach is to use functionality in **Torchvision** library, namely `ImageFolder`. It does all the preprocessing steps automatically, and also assigns labels to images according to the directory structure. We will see the example of using `ImageFolder` later in this course, once we start classifying our own cats and dogs images.\n",
    "\n",
    "> It is important to note that all images should be scaled to the same size. If your original images have different aspect ratios, you need to decide how to handle this scaling - either by cropping images, or by padding extra space.\n",
    "\n",
    "## Takeaway\n",
    "\n",
    "Neural networks work with tensors, and before training any models we need to convert our dataset into a set of tensors. This will often require reshaping. We have loaded training and test datasets, and we are ready to start training our first neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48436ba23936707534a03677b1edddd704d08d5a9542cfd6f4d878d1817756e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
